# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/Encoder.ipynb (unless otherwise specified).

__all__ = ['Encoder']

# Cell

import torch.nn as nn
import torch
from .scene_graph.scene_graph import SceneGraph
from torchvision.models import resnet34

# Cell

class Encoder(nn.Module):
    def __init__(self, dim = 256, mode=None):
        super().__init__()

        """
        Input:
            dim : final number of dimensions of the node and spatial embeddings

        Returns:
            Intialises a model which has node embeddimgs and spatial embeddings
        """

        self.dim=dim
        self.mode=mode
        self.resnet = resnet34(pretrained=True)
        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])

        self.scene_graph = SceneGraph(feature_dim=self.dim,
                                 output_dims=[self.dim,self.dim],
                                 downsample_rate=16,
                                 mode=self.mode)


#         if self.mode=="spatial":
#             print("freezing feature extractor encoder")
#             self.set_parameter_requires_grad()

    def set_parameter_requires_grad(self):
        self.feature_extractor.requires_grad = False

    def forward(self,
                feed_dict,
                rel_viewpoint=None):
        """
        Input:
            feed_dict: a dictionary containing list tensors containing images and bounding box data.
            Each element of the feed_dict corresponds to one elment of the batch.
            Inside each batch are contained ["image": Image tensor,
                                             "boxes":Bounding box tensor,
                                             bounding box
                                            ]
            mode: should be either 'node' or 'spatial' depending on what feature you want to extract
        """
        mode = self.mode

        image_features = self.feature_extractor(feed_dict["images"])
        outputs = self.scene_graph(image_features, feed_dict["objects_boxes"], feed_dict["objects"])

        return outputs
#         if mode=="node":
#             return outputs

#         if mode=="spatial" and rel_viewpoint is not None:
#             outputs = self.merge_pose_with_scene_embeddings(outputs,rel_viewpoint)
#             outputs = self.do_viewpoint_transformation(outputs)

#             return outputs

#         if mode=="spatial" and rel_viewpoint is None:
#             return outputs