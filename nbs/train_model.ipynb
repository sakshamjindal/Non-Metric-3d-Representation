{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import builtins\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from core.model.model import MoCo\n",
    "from core.dataloader import CLEVR_train, collate_boxes, CLEVR_train_onlyquery, collate_boxes_onlyquery\n",
    "from core.utils import compute_features, run_kmeans, AverageMeter, ProgressMeter, adjust_learning_rate, accuracy, save_checkpoint, DoublePool_O, store_to_pool, random_retrieve_topk, plot_query_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num-cluster'], dest='num_cluster', nargs=None, const=None, default='50,100,200', type=<class 'str'>, choices=None, help='number of clusters (should be less than equal to number of samples)', metavar=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Relational 2d Training')\n",
    "# parser.add_argument('data', metavar='DIR',\n",
    "#                     help='path to datasets root directory')\n",
    "parser.add_argument('-j', '--num-worker', default=1, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 1)')\n",
    "parser.add_argument('--epochs', default=200, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=16, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 16), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.03, type=float,\n",
    "                    metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--schedule', default=[120, 160], nargs='*', type=int,\n",
    "                    help='learning rate schedule (when to drop lr by 10x)')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum of SGD solver')\n",
    "parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)',\n",
    "                    dest='weight_decay')\n",
    "parser.add_argument('-p', '--print-freq', default=100, type=int,\n",
    "                    metavar='N', help='print iter frequency (default: 100)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "\n",
    "parser.add_argument('--seed', default=None, type=int,\n",
    "                    help='seed for initializing training. ')\n",
    "parser.add_argument(\"--gpu\", type=int, nargs='+', default=None, help='GPU id to use.')\n",
    "parser.add_argument('--warmup-epoch', default=10, type=int,\n",
    "                    help='number of warm-up epochs to only train with InfoNCE loss')\n",
    "parser.add_argument('--cos', action='store_true',\n",
    "                    help='use cosine lr schedule')\n",
    "parser.add_argument('--exp-dir', default='experiment_pcl', type=str,\n",
    "                    help='experiment directory to store tb logs and checkpoints')\n",
    "parser.add_argument('--num-cluster', default='50,100,200', type=str, \n",
    "                    help='number of clusters (should be less than equal to number of samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('./tb_logs'):\n",
    "    os.makedirs('./tb_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def setup_tb(exp_name):\n",
    "    tb_directory = os.path.join('./tb_logs', exp_name)\n",
    "    return SummaryWriter(tb_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def run_training(args):\n",
    "    \n",
    "#     parser = argparse.ArgumentParser(description='Relational 2d Training')\n",
    "    \n",
    "#     if default_args:\n",
    "#         args = parser.parse_args(default_args)\n",
    "#     else:\n",
    "#         args = parser.parse_args()\n",
    "        \n",
    "    tb_logger = setup_tb(args.exp_dir)\n",
    "    \n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                      'disable data parallelism.')\n",
    "        \n",
    "    args.num_cluster = args.num_cluster.split(',')\n",
    "    \n",
    "    if not os.path.exists(args.exp_dir):\n",
    "        os.mkdir(args.exp_dir)\n",
    "    if not os.path.exists(os.path.join('./tb_logs',args.exp_dir)):\n",
    "        os.mkdir(os.path.join('./tb_logs', args.exp_dir))\n",
    "    \n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    \n",
    "    gpu_devices = ','.join([str(id) for id in range(ngpus_per_node)])\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_devices\n",
    "    \n",
    "    best_acc = 0\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print('==> Preparing data..')\n",
    "    \n",
    "    traindir = os.path.join(args.data)\n",
    "    valdir = os.path.join(args.data[:-5] + 'v.txt')\n",
    "    moco_train_dataset = CLEVR_train(root_dir=traindir, hyp_N=args.hyp_N)\n",
    "    moco_train_loader = DataLoader(moco_train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_boxes)\n",
    "    \n",
    "    kmeans_train_dataset = CLEVR_train_onlyquery(root_dir=traindir, hyp_N=args.hyp_N)\n",
    "    kmeans_train_loader = DataLoader(kmeans_train_dataset, batch_size=5*args.batch_size, shuffle=False, collate_fn=collate_boxes_onlyquery)\n",
    "    \n",
    "    pool_size = len(moco_train_dataset)\n",
    "    \n",
    "    isnode = False\n",
    "    if args.mode=='node':\n",
    "        isnode = True\n",
    "        \n",
    "    pool_e_train = DoublePool_O(pool_size, isnode)\n",
    "    pool_g_train = DoublePool_O(pool_size, isnode)\n",
    "    \n",
    "    moco_val_dataset = CLEVR_train(root_dir=valdir, hyp_N=args.hyp_N)\n",
    "    moco_val_loader = DataLoader(moco_val_dataset, batch_size=1, shuffle=True, collate_fn=collate_boxes)\n",
    "    \n",
    "    pool_size = len(moco_val_dataset)\n",
    "    pool_e_val = DoublePool_O(pool_size, isnode)\n",
    "    pool_g_val = DoublePool_O(pool_size, isnode)\n",
    "\n",
    "    print('==> Making model..')\n",
    "\n",
    "    model = MoCo(mode=args.mode, r=args.moco_r)\n",
    "    #model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('The number of parameters of model is', num_params)\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    \n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "#             else:\n",
    "#                 # Map model to be loaded to specified single gpu.\n",
    "#                 loc = 'cuda:{}'.format(args.gpu)\n",
    "#                 checkpoint = torch.load(args.resume, map_location=loc)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "            \n",
    "    if args.use_pretrained:\n",
    "        if os.path.isfile(args.use_pretrained):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.use_pretrained))\n",
    "            state_dict = torch.load(args.use_pretrained)['state_dict']\n",
    "            del state_dict['queue']\n",
    "            del state_dict['queue_ptr']\n",
    "\n",
    "            model_dict = model.state_dict()\n",
    "            model_dict.update(state_dict)\n",
    "\n",
    "            model.load_state_dict(model_dict)\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.use_pretrained))\n",
    "            \n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "\n",
    "        cluster_result = None\n",
    "        \n",
    "        if epoch>=args.warmup_epoch:\n",
    "            # compute momentum features for center-cropped images\n",
    "            features = compute_features(kmeans_train_loader, model, args)         \n",
    "\n",
    "            # placeholder for clustering result\n",
    "            cluster_result = {'im2cluster':[],'centroids':[],'density':[]}\n",
    "            for num_cluster in args.num_cluster:\n",
    "                cluster_result['im2cluster'].append(torch.zeros(len(kmeans_train_dataset),dtype=torch.long).cuda())\n",
    "                cluster_result['centroids'].append(torch.zeros(int(num_cluster),256).cuda())\n",
    "                cluster_result['density'].append(torch.zeros(int(num_cluster)).cuda()) \n",
    "\n",
    "#             features[torch.norm(features,dim=1)>1.5] /= 2 #account for the few samples that are computed twice  \n",
    "            features = features.numpy()\n",
    "            cluster_result = run_kmeans(features,args)  #run kmeans clustering on master node\n",
    "                # save the clustering result\n",
    "                # torch.save(cluster_result,os.path.join(args.exp_dir, 'clusters_%d'%epoch))  \n",
    "\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "\n",
    "       \n",
    "        train(moco_train_loader, model, criterion, optimizer, epoch, args, cluster_result, tb_logger, pool_e_train, pool_g_train)\n",
    "    \n",
    "        \n",
    "   \n",
    "        if (epoch+1)%5==0:\n",
    "            val_retrieval(moco_val_loader, model, epoch, args, tb_logger, pool_e_val, pool_g_val)\n",
    "        if (epoch+1)%50==0:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best=False, filename='./tb_logs/{}/checkpoint_{}.pth.tar'.format(args.exp_dir, str(epoch)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, args, cluster_result=None, tb_logger=None, pool_e=None, pool_g=None):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    acc_inst = AverageMeter('Acc@Inst', ':6.2f')   \n",
    "    acc_proto = AverageMeter('Acc@Proto', ':6.2f')\n",
    "    \n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, acc_inst, acc_proto],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (feed_dict_q, feed_dict_k, metadata) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)  \n",
    "        \n",
    "                \n",
    "        # compute output\n",
    "        index = metadata[\"index\"]\n",
    "        output, target, output_proto, target_proto = model(feed_dict_q, feed_dict_k, metadata, cluster_result=cluster_result, index=index)\n",
    "        \n",
    "        \n",
    "        # InfoNCE loss\n",
    "        loss = criterion(output, target)  \n",
    "        \n",
    "        # ProtoNCE loss\n",
    "        if output_proto is not None:\n",
    "            loss_proto = 0\n",
    "            for proto_out,proto_target in zip(output_proto, target_proto):\n",
    "                loss_proto += criterion(proto_out, proto_target)  \n",
    "                accp = accuracy(proto_out, proto_target)[0] \n",
    "                acc_proto.update(accp[0], args.batch_size)\n",
    "                \n",
    "            # average loss across all sets of prototypes\n",
    "            loss_proto /= len(args.num_cluster) \n",
    "            loss += loss_proto   \n",
    "\n",
    "        losses.update(loss.item(), args.batch_size)\n",
    "        acc = accuracy(output, target)[0] \n",
    "        acc_inst.update(acc[0], args.batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # store to pool\n",
    "        store_to_pool(pool_e, pool_g, feed_dict_q, feed_dict_k, metadata, model, args)\n",
    "        model.train()\n",
    " \n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "    print(\"Logging to TB....\")\n",
    "    tb_logger.add_scalar('Train Acc Inst', acc_inst.avg, epoch)\n",
    "    tb_logger.add_scalar('Train Acc Prototype', acc_proto.avg, epoch)\n",
    "    tb_logger.add_scalar('Train Total Loss', losses.avg, epoch)\n",
    "    \n",
    "    if epoch % args.ret_freq == 0:\n",
    "        figures = random_retrieve_topk(args, pool_e, pool_g, imgs_to_view=3)\n",
    "        tb_logger.add_figure('Train Top10 Retrieval', figures, epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'exp_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-83bb7faf31c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"--exp-dir test_run\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-63abe8b08ad6>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         args = parser.parse_args()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtb_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_tb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'exp_dir'"
     ]
    }
   ],
   "source": [
    "run_training(args=\"--exp-dir test_run\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "def val_retrieval(val_loader, model, epoch, args, tb_logger=None, pool_e=None, pool_g=None):\n",
    "    model.eval()\n",
    "    for i, (feed_dict_q, feed_dict_k, metadata) in enumerate((val_loader)):\n",
    "        with torch.no_grad():\n",
    "            feat_q = model(feed_dict_q, None, metadata, is_eval=True)\n",
    "            feat_k = model(feed_dict_k, None, metadata, is_eval=True)\n",
    "\n",
    "            dim1 = feat_q.shape[0]\n",
    "            img_q = torch.zeros([dim1, 3, 256, 256])\n",
    "            img_k = torch.zeros([dim1, 3, 256, 256])\n",
    "\n",
    "\n",
    "            if args.mode=='node':\n",
    "                cnt = 0\n",
    "\n",
    "                for b in range(feed_dict_q[\"objects_boxes\"].shape[0]//args.hyp_N):\n",
    "                    for s in range(args.hyp_N):\n",
    "                        img_q[cnt] = feed_dict_q[\"images\"][b]\n",
    "                        img_k[cnt] = feed_dict_k[\"images\"][b]\n",
    "                        cnt += 1\n",
    "\n",
    "                pool_e.update(feat_q, img_q, feed_dict_q[\"objects_boxes\"], None)\n",
    "                pool_g.update(feat_k, img_k, feed_dict_k[\"objects_boxes\"], None)\n",
    "\n",
    "            else:\n",
    "                dim1 = feat_q.shape[0]\n",
    "                subj_q = torch.zeros([dim1, 4])\n",
    "                subj_k = torch.zeros([dim1, 4])\n",
    "                obj_q = torch.zeros([dim1, 4])\n",
    "                obj_k = torch.zeros([dim1, 4])\n",
    "\n",
    "                cnt = 0\n",
    "                for b in range(feed_dict_q[\"objects_boxes\"].shape[0]//args.hyp_N):\n",
    "                    for s in range(args.hyp_N):\n",
    "                        for o in range(args.hyp_N):\n",
    "                            img_q[cnt] = feed_dict_q[\"images\"][b]\n",
    "                            img_k[cnt] = feed_dict_k[\"images\"][b]\n",
    "                            cnt += 1\n",
    "\n",
    "                cnt = 0\n",
    "                for b in range(feed_dict_q[\"objects_boxes\"].shape[0]//args.hyp_N):\n",
    "                    for s in range(args.hyp_N):\n",
    "                        for o in range(args.hyp_N):\n",
    "                            start_idx = b*args.hyp_N\n",
    "                            subj_q[cnt] = feed_dict_q[\"objects_boxes\"][start_idx + s]\n",
    "                            obj_q[cnt] = feed_dict_q[\"objects_boxes\"][start_idx + o]\n",
    "                            cnt += 1\n",
    "\n",
    "\n",
    "                cnt = 0\n",
    "                for b in range(feed_dict_q[\"objects_boxes\"].shape[0]//args.hyp_N):\n",
    "                    for s in range(args.hyp_N):\n",
    "                        for o in range(args.hyp_N):\n",
    "                            start_idx = b*args.hyp_N\n",
    "                            subj_k[cnt] = feed_dict_k[\"objects_boxes\"][start_idx + s]\n",
    "                            obj_k[cnt] = feed_dict_k[\"objects_boxes\"][start_idx + o]\n",
    "                            cnt += 1\n",
    "\n",
    "                pool_e.update(feat_q, img_q, subj_q, obj_q)\n",
    "                pool_g.update(feat_k, img_k, subj_k, obj_k)\n",
    "                \n",
    "    figures = random_retrieve_topk(args, pool_e, pool_g, imgs_to_view=5)\n",
    "    tb_logger.add_figure('Validation Top10 Retrieval', figures, epoch)\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "disco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
