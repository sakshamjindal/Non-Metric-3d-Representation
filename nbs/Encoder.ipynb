{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with open(\"../feed_dict.json\",\"rb\") as file:\n",
    "    feed_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_q = feed_dict\n",
    "feed_dict_k = feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "feed_dict_q[\"view\"] = torch.rand((feed_dict[\"image\"].size(0),1,7))\n",
    "feed_dict_k[\"view\"] = torch.rand((feed_dict[\"image\"].size(0),1,7))\n",
    "\n",
    "feed_dict[\"images\"] = feed_dict[\"image\"]\n",
    "feed_dict[\"objects_boxes\"] = feed_dict[\"objects\"]\n",
    "feed_dict[\"objects\"] = feed_dict[\"objects_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from core.scene_graph.scene_graph import SceneGraph\n",
    "from torchvision.models import resnet34\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "            dim : final number of dimensions of the node and spatial embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Intialises a model which has node embeddimgs and spatial embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.dim = dim\n",
    "        self.resnet = resnet34(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])\n",
    "        \n",
    "        self.scene_graph = SceneGraph(feature_dim=self.dim, \n",
    "                                 output_dims=[self.dim,self.dim],\n",
    "                                 downsample_rate=16)\n",
    "        \n",
    "        \n",
    "    def forward(self,feed_dict):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            feed_dict: a dictionary containing list tensors containing images and bounding box data. \n",
    "            Each element of the feed_dict corresponds to one elment of the batch. \n",
    "            Inside each batch are contained [\"image\": Image tensor, \n",
    "                                             \"boxes\":Bounding box tensor,\n",
    "                                             bounding box\n",
    "                                            ]\n",
    "        \"\"\"\n",
    "        num_batch = feed_dict[\"images\"].shape[0]\n",
    "        num_total_nodes = feed_dict[\"objects\"].sum().item()\n",
    "        \n",
    "        image_features = self.feature_extractor(feed_dict[\"images\"])\n",
    "        outputs = self.scene_graph(image_features, feed_dict[\"objects_boxes\"], feed_dict[\"objects\"])\n",
    "        \n",
    "        node_features = outputs[0][0]\n",
    "        for num in range(1,num_batch):\n",
    "            node_features = torch.cat([node_features, outputs[num][0]], dim =0)\n",
    "        \n",
    "        # To be implemented\n",
    "        spatial_features = None\n",
    "        \n",
    "        return outputs, node_features, spatial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, node_features, spatial_features = encoder(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256, 384])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict[\"images\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "disco"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
