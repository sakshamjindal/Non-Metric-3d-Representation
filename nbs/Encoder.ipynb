{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from core.model.scene_graph.scene_graph import SceneGraph\n",
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloader import CLEVR_train, collate_boxes\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised..... 234  files...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CLEVR_train(root_dir='/home/mprabhud/dataset/clevr_lang/npys/ab_5t.txt')\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    feed_dict_q, feed_dict_k, metadata = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_q[\"images\"] = feed_dict_k[\"images\"].cuda()\n",
    "feed_dict_k[\"images\"] = feed_dict_k[\"images\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim = 256, mode=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "            dim : final number of dimensions of the node and spatial embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Intialises a model which has node embeddimgs and spatial embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim=dim\n",
    "        self.mode=mode\n",
    "        self.resnet = resnet34(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])\n",
    "        \n",
    "        self.scene_graph = SceneGraph(feature_dim=self.dim, \n",
    "                                 output_dims=[self.dim,self.dim],\n",
    "                                 downsample_rate=16,\n",
    "                                 mode=self.mode)\n",
    "\n",
    "        self.spatial_viewpoint_transformation = nn.Sequential(nn.Linear(263,512),\n",
    "                                                        nn.ReLU(),\n",
    "                                                        nn.Linear(512,self.dim))\n",
    "        \n",
    "        if self.mode==\"spatial\":\n",
    "            self.set_parameter_requires_grad()\n",
    "            \n",
    "    def set_parameter_requires_grad(self):\n",
    "        self.feature_extractor.requires_grad = False\n",
    "        \n",
    "    def merge_pose_with_scene_embeddings(self,\n",
    "                                     scene_embeddings,\n",
    "                                     view=None):\n",
    "        '''\n",
    "        Input\n",
    "            scene_embeddings: output of scene_graph module. A list of of tensors containing node and\n",
    "                              spatial embeddings of each batch element\n",
    "            view : a tensor of size [batch, 1, 7] containing information of relative egomotion\n",
    "                   between the two camera viewpoints\n",
    "            transform_node and transform spatial: boolean flags whether to do any transformation on nodes or not\n",
    "        Output\n",
    "            scene_embeddings: concatenated with pose vectors\n",
    "        '''\n",
    "\n",
    "        for batch_ind,(_, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "            num_obj_x = spatial_embeddings.shape[0]\n",
    "            num_obj_y = spatial_embeddings.shape[1]\n",
    "\n",
    "            # Broadcast view to spatial embedding dimension\n",
    "            view_spatial = view[batch_ind].unsqueeze(0).repeat(num_obj_x, num_obj_y, 1)\n",
    "            # Concatenate with visual embeddings\n",
    "            pose_with_features = torch.cat((view_spatial,spatial_embeddings), dim=2)\n",
    "            # Reassign the scene embeddings\n",
    "            scene_embeddings[batch_ind][1] = pose_with_features\n",
    "\n",
    "            ### To Do : Write some assertion test : (Saksham)\n",
    "\n",
    "        return scene_embeddings\n",
    "\n",
    "    def do_viewpoint_transformation(self,\n",
    "                                    scene_embeddings,\n",
    "                                    transform_node=True,\n",
    "                                    transform_spatial=False):\n",
    "\n",
    "        '''\n",
    "        Input:\n",
    "            scene_embeddings: output of scene_graph module concatenated with pose. A list of of tensors containing node and\n",
    "                              spatial embeddings of each batch element\n",
    "            transform_node and transform spatial: boolean flags whether to do any transformation on nodes or not\n",
    "        Output:\n",
    "            scene_embeddings: viewpoint transformed embeddings\n",
    "        '''\n",
    "        for ind,(_, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "            # Do viewpoint transformation on spatial embeddings\n",
    "            scene_embeddings[ind][1] = self.spatial_viewpoint_transformation(scene_embeddings[ind][1])\n",
    "\n",
    "        return scene_embeddings\n",
    "\n",
    "    def forward(self,\n",
    "                feed_dict,\n",
    "                rel_viewpoint=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            feed_dict: a dictionary containing list tensors containing images and bounding box data.\n",
    "            Each element of the feed_dict corresponds to one elment of the batch.\n",
    "            Inside each batch are contained [\"image\": Image tensor,\n",
    "                                             \"boxes\":Bounding box tensor,\n",
    "                                             bounding box\n",
    "                                            ]\n",
    "            mode: should be either 'node' or 'spatial' depending on what feature you want to extract\n",
    "        \"\"\"\n",
    "        mode = self.mode\n",
    "        num_batch = feed_dict[\"images\"].shape[0]\n",
    "        num_total_nodes = feed_dict[\"objects\"].sum().item()\n",
    "\n",
    "        image_features = self.feature_extractor(feed_dict[\"images\"])\n",
    "        outputs = self.scene_graph(image_features, feed_dict[\"objects_boxes\"], feed_dict[\"objects\"])\n",
    "\n",
    "        if mode==\"node\":\n",
    "            return outputs\n",
    "\n",
    "        if mode==\"spatial\" and rel_viewpoint is not None:\n",
    "            outputs = self.merge_pose_with_scene_embeddings(outputs,rel_viewpoint)\n",
    "            outputs = self.do_viewpoint_transformation(outputs)\n",
    "            \n",
    "            return outputs\n",
    "            \n",
    "        if mode==\"spatial\" and rel_viewpoint is None:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Mode** : Node Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(mode=\"node\")\n",
    "enoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_ = feed_dict_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_outputs_ = encoder(feed_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_ = encoder.feature_extractor(feed_dict_[\"images\"])\n",
    "scene_graph_output = encoder.scene_graph(image_features_, feed_dict_[\"objects_boxes\"], feed_dict_[\"objects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ind = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([2, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scene_graph_output), scene_graph_output[batch_ind][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Mode** : Spatial Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(mode=\"spatial\")\n",
    "enoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_viewpoint_ = metadata[\"rel_viewpoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ading viewpoint information to spatial features\n",
      "0\n",
      "Adding pose to spatial embeddings\n",
      "1\n",
      "Adding pose to spatial embeddings\n",
      "2\n",
      "Adding pose to spatial embeddings\n",
      "3\n",
      "Adding pose to spatial embeddings\n",
      "4\n",
      "Adding pose to spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n"
     ]
    }
   ],
   "source": [
    "spatial_outputs_ = encoder(feed_dict_, rel_viewpoint= rel_viewpoint_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([2, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spatial_outputs_), spatial_outputs_[batch_ind][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([2, 2, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spatial_outputs_), spatial_outputs_[batch_ind][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_k_ = feed_dict_k\n",
    "feed_dict_q_ = feed_dict_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(mode=\"node\")\n",
    "enoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_k_ = encoder(feed_dict_k_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_q_ = encoder(feed_dict_q_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_embeddings(output_k, output_q, mode = \"node\"):\n",
    "    \n",
    "    if mode==\"node\":\n",
    "        mode = 0\n",
    "    elif mode==\"spatial\":\n",
    "        mode = 1\n",
    "    else:\n",
    "        raise ValueError(\"Mode should be either node or spatial\")\n",
    "    \n",
    "    num_batch = len(output_k)\n",
    "    assert num_batch==len(output_q)   \n",
    "    \n",
    "    output_q_rearrange = []\n",
    "    \n",
    "    for batch_ind in range(num_batch):\n",
    "        \n",
    "        num_obj_in_batch = output_k[batch_ind][0].shape[0]\n",
    "        assert num_obj_in_batch==output_q[batch_ind][0].shape[0]\n",
    "        \n",
    "        if mode==\"spatial\":\n",
    "            assert num_obj_in_batch==output_q[batch_ind][1].shape[0]\n",
    "            assert num_obj_in_batch==output_q[batch_ind][1].shape[1]\n",
    "            assert output_k[batch_ind][1].shape[0]==output_k[batch_ind][1].shape[0]\n",
    "            assert output_k[batch_ind][1].shape[1]==output_k[batch_ind][1].shape[1]\n",
    "            assert output_k[batch_ind][1].shape[0]==output_k[batch_ind][1].shape[1]\n",
    "            assert output_k[batch_ind][1].shape[1]==output_k[batch_ind][1].shape[0]\n",
    "            \n",
    "        #flatten the node features only - \n",
    "        output_k[batch_ind][0] = output_k[batch_ind][0].view(-1,256)\n",
    "        output_q[batch_ind][0] = output_q[batch_ind][0].view(-1,256)\n",
    "        \n",
    "        \n",
    "        #form two pool from node features for nearest neighbour search\n",
    "        pool_e = output_k[batch_ind][0].clone().detach().cpu()\n",
    "        pool_g = output_q[batch_ind][0].clone().detach().cpu()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            knn_e = NearestNeighbors(n_neighbors= num_obj_in_batch, metric=\"euclidean\")\n",
    "            knn_g = NearestNeighbors(n_neighbors= num_obj_in_batch, metric=\"euclidean\")\n",
    "\n",
    "            knn_g.fit(pool_g)\n",
    "            knn_e.fit(pool_e)\n",
    "            \n",
    "            paired = []\n",
    "            pairs = []\n",
    "            for index in range(num_obj_in_batch):  \n",
    "\n",
    "                #fit knn on each of the object \n",
    "                _, indices_e = knn_g.kneighbors(torch.reshape(pool_e[index], (1,-1)).detach().cpu())\n",
    "                indices_e = list(indices_e.flatten())\n",
    "                for e in indices_e:\n",
    "                    if e not in paired:\n",
    "                        paired.append(e)\n",
    "                        pairs.append(e)\n",
    "                        break\n",
    "        \n",
    "        print(pairs)\n",
    "        #rearranging the matched in output_q based on pair formed\n",
    "        \n",
    "    \n",
    "        #Rearranging the node_features in output_q based on pair formed\n",
    "        assert num_obj_in_batch == len(pairs)\n",
    "        \n",
    "        node_pool_rearranged = torch.zeros(pool_e.shape[0], 256)\n",
    "        for index_node in range(num_obj_in_batch):\n",
    "            pair_mapping_obj = pairs[index_node]\n",
    "            node_pool_rearranged[index_node] = output_q[batch_ind][0][pair_mapping_obj].clone()\n",
    "        \n",
    "        output_q[batch_ind][0] = node_pool_rearranged.cuda()\n",
    "        \n",
    "        #If mode is spatial : also repair the spatial embeddings\n",
    "        if mode==\"spatial\":\n",
    "            spatial_pool_rearranged = torch.zeros(pool_e.shape[0], pool_e.shape[0], 256)\n",
    "            for index_subj in range(num_obj_in_batch):\n",
    "                for index_obj in range(num_obj_in_batch):\n",
    "                    pair_mapping_subj = pairs[index_subj]\n",
    "                    pair_mapping_obj = pairs[index_obj]\n",
    "                    spatial_pool_rearranged[index_subj][index_obj] = output_q[batch_ind][1][pair_mapping_subj][pair_mapping_obj].clone()\n",
    "                    \n",
    "            output_q[batch_ind][1] = spatial_pool_rearranged\n",
    "        \n",
    "    return output_k, output_q    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "rearranged_output_k, rearranged_output_q = pair_embeddings(output_k_, output_q_, mode = \"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearranged_output_k==output_k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearranged_output_q==output_q_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching the spatial embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(mode=\"spatial\")\n",
    "enoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ading viewpoint information to spatial features\n",
      "0\n",
      "Adding pose to spatial embeddings\n",
      "1\n",
      "Adding pose to spatial embeddings\n",
      "2\n",
      "Adding pose to spatial embeddings\n",
      "3\n",
      "Adding pose to spatial embeddings\n",
      "4\n",
      "Adding pose to spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n"
     ]
    }
   ],
   "source": [
    "output_k__ = encoder(feed_dict_k_)\n",
    "output_q__ = encoder(feed_dict_q_, rel_viewpoint_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "rearranged_output_k_, rearranged_output_q_= pair_embeddings(output_k__, output_q__, mode = \"spatial\")\n",
    "\n",
    "\n",
    "#Code breaking resolve later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the embeddings across batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_features_across_batch(output_feature_list, mode=\"node\"):\n",
    "\n",
    "    num_batch = len(output_feature_list)\n",
    "    if mode==\"node\":  \n",
    "        node_features = output_feature_list[0][0].view(-1,256)\n",
    "\n",
    "        for num in range(1,num_batch):\n",
    "            node_features = torch.cat([node_features, output_feature_list[num][0]], dim =0)\n",
    "        \n",
    "        return node_features\n",
    "    \n",
    "    if mode==\"spatial\":\n",
    "        spatial_features = output_feature_list[0][1].view(-1,256)\n",
    "\n",
    "        for num in range(1, num_batch):\n",
    "            spatial_features = torch.cat([spatial_features, output_feature_list[num][1].view(-1,256)], dim =0)\n",
    "            \n",
    "        return spatial_features\n",
    "    \n",
    "    raise ValueError(\"Training mode not defined properly. It should be either 'node' or 'spatial'.\" )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_output_k = stack_features_across_batch(rearranged_output_k, mode=\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_output_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_output_k[3] == rearranged_output_k[1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_output_k_ = stack_features_across_batch(rearranged_output_k_, mode=\"spatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_output_k_.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "disco"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
