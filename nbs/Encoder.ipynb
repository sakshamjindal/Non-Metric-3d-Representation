{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from core.model.scene_graph.scene_graph import SceneGraph\n",
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloader import GQNDataset_pdisco, collate_boxes\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised..... 27495  files...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GQNDataset_pdisco(root_dir='/home/mprabhud/dataset/clevr_veggies/npys/be_lt.txt')\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    feed_dict_q, feed_dict_k, metadata = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_q[\"images\"] = feed_dict_k[\"images\"].cuda()\n",
    "feed_dict_k[\"images\"] = feed_dict_k[\"images\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "            dim : final number of dimensions of the node and spatial embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Intialises a model which has node embeddimgs and spatial embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.resnet = resnet34(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])\n",
    "        \n",
    "        self.scene_graph = SceneGraph(feature_dim=self.dim, \n",
    "                                 output_dims=[self.dim,self.dim],\n",
    "                                 downsample_rate=16)\n",
    "        \n",
    "        self.node_viewpoint_transformation = nn.Sequential(nn.Linear(263,512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512,self.dim))\n",
    "\n",
    "        self.spatial_viewpoint_transformation = nn.Sequential(nn.Linear(263,512),\n",
    "                                                        nn.ReLU(),\n",
    "                                                        nn.Linear(512,self.dim))\n",
    "        \n",
    "    def merge_pose_with_scene_embeddings(self,\n",
    "                                     scene_embeddings, \n",
    "                                     view=None):\n",
    "        '''\n",
    "        Input\n",
    "            scene_embeddings: output of scene_graph module. A list of of tensors containing node and\n",
    "                              spatial embeddings of each batch element\n",
    "            view : a tensor of size [batch, 1, 7] containing information of relative egomotion\n",
    "                   between the two camera viewpoints\n",
    "            transform_node and transform spatial: boolean flags whether to do any transformation on nodes or not\n",
    "        Output\n",
    "            scene_embeddings: concatenated with pose vectors\n",
    "        '''\n",
    "\n",
    "        for batch_ind,(_, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "            print(batch_ind)\n",
    "            num_obj_x = spatial_embeddings.shape[0]\n",
    "            num_obj_y = spatial_embeddings.shape[1]\n",
    "\n",
    "            print(\"Adding pose to spatial embeddings\")\n",
    "            # Broadcast view to spatial embedding dimension\n",
    "            view_spatial = view[batch_ind].unsqueeze(0).repeat(num_obj_x, num_obj_y, 1)\n",
    "            # Concatenate with visual embeddings\n",
    "            pose_with_features = torch.cat((view_spatial,spatial_embeddings), dim=2)\n",
    "            # Reassign the scene embeddings\n",
    "            scene_embeddings[batch_ind][1] = pose_with_features\n",
    "\n",
    "            ### To Do : Write some assertion test : (Saksham)\n",
    "\n",
    "        return scene_embeddings\n",
    "\n",
    "    def do_viewpoint_transformation(self,\n",
    "                                    scene_embeddings,                                     \n",
    "                                    transform_node=True, \n",
    "                                    transform_spatial=False):\n",
    "\n",
    "        '''\n",
    "        Input:\n",
    "            scene_embeddings: output of scene_graph module concatenated with pose. A list of of tensors containing node and\n",
    "                              spatial embeddings of each batch element\n",
    "            transform_node and transform spatial: boolean flags whether to do any transformation on nodes or not\n",
    "        Output:\n",
    "            scene_embeddings: viewpoint transformed embeddings\n",
    "        '''\n",
    "        for ind,(_, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "            # Do viewpoint transformation on spatial embeddings\n",
    "            print(\"viewpoint transform on spatial embeddings\")\n",
    "            scene_embeddings[ind][1] = self.spatial_viewpoint_transformation(scene_embeddings[ind][1])\n",
    "\n",
    "        return scene_embeddings \n",
    "        \n",
    "    def forward(self,\n",
    "                feed_dict,\n",
    "                mode=\"node\",\n",
    "                rel_viewpoint=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            feed_dict: a dictionary containing list tensors containing images and bounding box data. \n",
    "            Each element of the feed_dict corresponds to one elment of the batch. \n",
    "            Inside each batch are contained [\"image\": Image tensor, \n",
    "                                             \"boxes\":Bounding box tensor,\n",
    "                                             bounding box\n",
    "                                            ]\n",
    "            mode: should be either 'node' or 'spatial' depending on what feature you want to extract\n",
    "        \"\"\"\n",
    "        num_batch = feed_dict[\"images\"].shape[0]\n",
    "        num_total_nodes = feed_dict[\"objects\"].sum().item()\n",
    "        \n",
    "        image_features = self.feature_extractor(feed_dict[\"images\"])\n",
    "        outputs = self.scene_graph(image_features, feed_dict[\"objects_boxes\"], feed_dict[\"objects\"], mode=mode)\n",
    "        \n",
    "        if mode==\"node\":\n",
    "            #Flatten the node embeddings\n",
    "            node_features = outputs[0][0]\n",
    "            for num in range(1,num_batch):\n",
    "                node_features = torch.cat([node_features, outputs[num][0]], dim =0) \n",
    "\n",
    "            return outputs, node_features\n",
    "        \n",
    "        if mode==\"spatial\" and rel_viewpoint is not None:\n",
    "            print(\"Ading viewpoint information to spatial features\")\n",
    "            outputs = self.merge_pose_with_scene_embeddings(outputs,rel_viewpoint)\n",
    "            outputs = self.do_viewpoint_transformation(outputs)\n",
    "        \n",
    "        #Flattent the spatial embeddings\n",
    "        spatial_features = outputs[0][1].reshape(-1,256)\n",
    "\n",
    "        for num in range(1, num_batch):\n",
    "            spatial_features = torch.cat([spatial_features, outputs[num][1].reshape(-1,256)], dim =0)\n",
    "        \n",
    "        return outputs, spatial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_ = feed_dict_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Mode** : Node Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_, node_features_ = encoder(feed_dict_, mode=\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_viewpoint_ = metadata[\"rel_viewpoint\"]\n",
    "rel_viewpoint_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_ = encoder.feature_extractor(feed_dict_[\"images\"])\n",
    "scene_graph_output = encoder.scene_graph(image_features_, feed_dict_[\"objects_boxes\"], feed_dict_[\"objects\"], mode=\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ind = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([3, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scene_graph_output), scene_graph_output[batch_ind][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Mode** : Spatial Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ading viewpoint information to spatial features\n",
      "0\n",
      "Adding pose to spatial embeddings\n",
      "1\n",
      "Adding pose to spatial embeddings\n",
      "2\n",
      "Adding pose to spatial embeddings\n",
      "3\n",
      "Adding pose to spatial embeddings\n",
      "4\n",
      "Adding pose to spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n"
     ]
    }
   ],
   "source": [
    "output_, spatial_features_ = encoder(feed_dict_, mode=\"spatial\", rel_viewpoint= rel_viewpoint_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_features = output_[0][1].reshape(-1,256)\n",
    "\n",
    "for num in range(1,len(output)):\n",
    "    spatial_features = torch.cat([spatial_features, output_[num][1].reshape(-1,256)], dim =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pose_with_scene = encoder.merge_pose_with_scene_embeddings(scene_graph_output, rel_viewpoint)\n",
    "merged_pose_with_scene[batch_ind][0].shape, merged_pose_with_scene[batch_ind][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_transformed_scene = encoder.do_viewpoint_transformation(merged_pose_with_scene, True, False)\n",
    "view_transformed_scene[batch_ind][0].shape, view_transformed_scene[batch_ind][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = encoder(feed_dict, rel_viewpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs , node_features, spatial_features = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_features=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict[\"objects\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "disco"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
