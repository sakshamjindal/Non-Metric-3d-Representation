{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from core.scene_graph.scene_graph import SceneGraph\n",
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloader import GQNDataset_pdisco, collate_boxes\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised..... 27495  files...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GQNDataset_pdisco(root_dir='/home/mprabhud/dataset/clevr_veggies/npys/be_lt.txt')\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    feed_dict_q, feed_dict_k, metadata = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_q[\"images\"] = feed_dict_k[\"images\"].cuda()\n",
    "feed_dict_k[\"images\"] = feed_dict_k[\"images\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "            dim : final number of dimensions of the node and spatial embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Intialises a model which has node embeddimgs and spatial embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.resnet = resnet34(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])\n",
    "        \n",
    "        self.scene_graph = SceneGraph(feature_dim=self.dim, \n",
    "                                 output_dims=[self.dim,self.dim],\n",
    "                                 downsample_rate=16)\n",
    "        \n",
    "        self.node_viewpoint_transformation = nn.Sequential(nn.Linear(263,512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512,self.dim))\n",
    "\n",
    "        self.spatial_viewpoint_transformation = nn.Sequential(nn.Linear(263,512),\n",
    "                                                        nn.ReLU(),\n",
    "                                                        nn.Linear(512,self.dim))\n",
    "        \n",
    "    def merge_pose_with_scene_embeddings(self,\n",
    "                                     scene_embeddings, \n",
    "                                     view=None, \n",
    "                                     transform_node=True, \n",
    "                                     transform_spatial=False):\n",
    "        '''\n",
    "        Input\n",
    "            scene_embeddings: output of scene_graph module. A list of of tensors containing node and\n",
    "                              spatial embeddings of each batch element\n",
    "            view : a tensor of size [batch, 1, 7] containing information of relative egomotion\n",
    "                   between the two camera viewpoints\n",
    "            transform_node and transform spatial: boolean flags whether to do any transformation on nodes or not\n",
    "        Output\n",
    "            scene_embeddings: concatenated with pose vectors\n",
    "        '''\n",
    "    \n",
    "        if view is None:\n",
    "            raise NotImplementedError(\"Wrong Implementation\")\n",
    "\n",
    "        for batch_ind,(node_embeddings, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "\n",
    "            if transform_node:\n",
    "                print(\"Node: Pose with Node Concat :  Batch Ind: {}\".format(batch_ind))\n",
    "                num_objects = node_embeddings.shape[0]\n",
    "                # Broadcast view to visual embedding dimension\n",
    "                view_visual = view[batch_ind].repeat(num_objects,1)\n",
    "                # Concatenate with visual embeddings\n",
    "                pose_with_features = torch.cat((view_visual,node_embeddings), dim=1)\n",
    "                # Reassign the scene embeddings\n",
    "                scene_embeddings[batch_ind][0] = pose_with_features\n",
    "\n",
    "            if transform_spatial:\n",
    "                num_obj_x = spatial_embeddings.shape[0]\n",
    "                num_obj_y = spatial_embeddings.shape[1]\n",
    "\n",
    "                # Broadcast view to spatial embedding dimension\n",
    "                view_spatial = view[batch_ind].unsqueeze(0).repeat(num_obj_x, num_obj_y, 1)\n",
    "                # Concatenate with visual embeddings\n",
    "                pose_with_features = torch.cat((view_spatial,spatial_embeddings), dim=2)\n",
    "                # Reassign the scene embeddings\n",
    "                scene_embeddings[batch_ind][1] = pose_with_features\n",
    "\n",
    "            ### To Do : Write some assertion test : (Saksham)\n",
    "\n",
    "        return scene_embeddings\n",
    "\n",
    "    def do_viewpoint_transformation(self,\n",
    "                                    scene_embeddings,                                     \n",
    "                                    transform_node=True, \n",
    "                                    transform_spatial=False):\n",
    "\n",
    "        '''\n",
    "        Input:\n",
    "            scene_embeddings: output of scene_graph module concatenated with pose. A list of of tensors containing node and\n",
    "                              spatial embeddings of each batch element\n",
    "            transform_node and transform spatial: boolean flags whether to do any transformation on nodes or not\n",
    "        Output:\n",
    "            scene_embeddings: viewpoint transformed embeddings\n",
    "        '''\n",
    "\n",
    "        for ind,(node_embeddings, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "            if transform_node:\n",
    "                print(\"Node: Transformation:  Batch Ind: {}\".format(ind))\n",
    "                # Do viewpoint transformation on visual embeddings\n",
    "                scene_embeddings[ind][0] = self.node_viewpoint_transformation(scene_embeddings[ind][0])\n",
    "\n",
    "            if transform_spatial:\n",
    "                # Do viewpoint transformation on spatial embeddings\n",
    "                scene_embeddings[ind][1] = self.spatial_viewpoint_transformation(scene_embeddings[ind][1])\n",
    "\n",
    "        return scene_embeddings \n",
    "        \n",
    "        \n",
    "    def forward(self,feed_dict, rel_viewpoint=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            feed_dict: a dictionary containing list tensors containing images and bounding box data. \n",
    "            Each element of the feed_dict corresponds to one elment of the batch. \n",
    "            Inside each batch are contained [\"image\": Image tensor, \n",
    "                                             \"boxes\":Bounding box tensor,\n",
    "                                             bounding box\n",
    "                                            ]\n",
    "        \"\"\"\n",
    "        num_batch = feed_dict[\"images\"].shape[0]\n",
    "        num_total_nodes = feed_dict[\"objects\"].sum().item()\n",
    "        \n",
    "        image_features = self.feature_extractor(feed_dict[\"images\"])\n",
    "        outputs = self.scene_graph(image_features, feed_dict[\"objects_boxes\"], feed_dict[\"objects\"])\n",
    "        \n",
    "        if rel_viewpoint is not None:\n",
    "            print(\"Viewpoint Transformation of Node feature vectors\")\n",
    "            outputs = self.merge_pose_with_scene_embeddings(outputs,rel_viewpoint, True, False)\n",
    "            outputs = self.do_viewpoint_transformation(outputs, True, False)\n",
    "        \n",
    "        node_features = outputs[0][0]\n",
    "        for num in range(1,num_batch):\n",
    "            node_features = torch.cat([node_features, outputs[num][0]], dim =0)\n",
    "        \n",
    "        # To be implemented\n",
    "        spatial_features = None\n",
    "        \n",
    "        return outputs, node_features, spatial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, node_features, spatial_features = encoder(feed_dict_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = feed_dict_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_viewpoint = metadata[\"rel_viewpoint\"]\n",
    "rel_viewpoint.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = encoder.feature_extractor(feed_dict[\"images\"])\n",
    "scene_graph_output = encoder.scene_graph(image_features, feed_dict[\"objects_boxes\"], feed_dict[\"objects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ind = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([3, 256]), torch.Size([3, 3, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scene_graph_output),scene_graph_output[batch_ind][0].shape, scene_graph_output[batch_ind][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: Pose with Node Concat :  Batch Ind: 0\n",
      "Node: Pose with Node Concat :  Batch Ind: 1\n",
      "Node: Pose with Node Concat :  Batch Ind: 2\n",
      "Node: Pose with Node Concat :  Batch Ind: 3\n",
      "Node: Pose with Node Concat :  Batch Ind: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 263]), torch.Size([3, 3, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_pose_with_scene = encoder.merge_pose_with_scene_embeddings(scene_graph_output, rel_viewpoint)\n",
    "merged_pose_with_scene[batch_ind][0].shape, merged_pose_with_scene[batch_ind][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: Transformation:  Batch Ind: 0\n",
      "Node: Transformation:  Batch Ind: 1\n",
      "Node: Transformation:  Batch Ind: 2\n",
      "Node: Transformation:  Batch Ind: 3\n",
      "Node: Transformation:  Batch Ind: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 256]), torch.Size([3, 3, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_transformed_scene = encoder.do_viewpoint_transformation(merged_pose_with_scene, True, False)\n",
    "view_transformed_scene[batch_ind][0].shape, view_transformed_scene[batch_ind][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewpoint Transformation of Node feature vectors\n",
      "Node: Pose with Node Concat :  Batch Ind: 0\n",
      "Node: Pose with Node Concat :  Batch Ind: 1\n",
      "Node: Pose with Node Concat :  Batch Ind: 2\n",
      "Node: Pose with Node Concat :  Batch Ind: 3\n",
      "Node: Pose with Node Concat :  Batch Ind: 4\n",
      "Node: Transformation:  Batch Ind: 0\n",
      "Node: Transformation:  Batch Ind: 1\n",
      "Node: Transformation:  Batch Ind: 2\n",
      "Node: Transformation:  Batch Ind: 3\n",
      "Node: Transformation:  Batch Ind: 4\n"
     ]
    }
   ],
   "source": [
    "outputs = encoder(feed_dict, rel_viewpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs , node_features, spatial_features = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_features=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict[\"objects\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "disco"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
