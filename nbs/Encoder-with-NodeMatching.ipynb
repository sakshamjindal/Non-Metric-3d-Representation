{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from core.model.scene_graph.scene_graph import SceneGraph\n",
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloader import CLEVR_train, collate_boxes\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised..... 234  files...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CLEVR_train(root_dir='/home/mprabhud/dataset/clevr_lang/npys/ab_5t.txt')\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    feed_dict_q, feed_dict_k, metadata = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_q[\"images\"] = feed_dict_k[\"images\"].cuda()\n",
    "feed_dict_k[\"images\"] = feed_dict_k[\"images\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "            dim : final number of dimensions of the node and spatial embeddings\n",
    "        \n",
    "        Returns:\n",
    "            Intialises a model which has node embeddimgs and spatial embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.resnet = resnet34(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet.children())[:-3])\n",
    "        \n",
    "        self.scene_graph = SceneGraph(feature_dim=self.dim, \n",
    "                                 output_dims=[self.dim,self.dim],\n",
    "                                 downsample_rate=16)\n",
    "        \n",
    "        self.node_viewpoint_transformation = nn.Sequential(nn.Linear(263,512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512,self.dim))\n",
    "\n",
    "        self.spatial_viewpoint_transformation = nn.Sequential(nn.Linear(263,512),\n",
    "                                                        nn.ReLU(),\n",
    "                                                        nn.Linear(512,self.dim))\n",
    "        \n",
    "    def merge_pose_with_scene_embeddings(self,\n",
    "                                     scene_embeddings, \n",
    "                                     view=None):\n",
    "        '''\n",
    "        Input\n",
    "            scene_embeddings: output of scene_graph module. A list of of tensors containing node and\n",
    "                              spatial embeddings of each batch element\n",
    "            view : a tensor of size [batch, 1, 7] containing information of relative egomotion\n",
    "                   between the two camera viewpoints\n",
    "            transform_node and transform spatial: boolean flags whether to do any transformation on nodes or not\n",
    "        Output\n",
    "            scene_embeddings: concatenated with pose vectors\n",
    "        '''\n",
    "\n",
    "        for batch_ind, spatial_embeddings in enumerate(scene_embeddings):\n",
    "            print(batch_ind)\n",
    "            num_obj_x = spatial_embeddings.shape[0]\n",
    "            num_obj_y = spatial_embeddings.shape[1]\n",
    "\n",
    "            print(\"Adding pose to spatial embeddings\")\n",
    "            # Broadcast view to spatial embedding dimension\n",
    "            view_spatial = view[batch_ind].unsqueeze(0).repeat(num_obj_x, num_obj_y, 1)\n",
    "            # Concatenate with visual embeddings\n",
    "            pose_with_features = torch.cat((view_spatial,spatial_embeddings), dim=2)\n",
    "            # Reassign the scene embeddings\n",
    "            scene_embeddings[batch_ind] = pose_with_features\n",
    "\n",
    "            ### To Do : Write some assertion test : (Saksham)\n",
    "\n",
    "        return scene_embeddings\n",
    "\n",
    "    def do_viewpoint_transformation(self,\n",
    "                                    scene_embeddings,                                     \n",
    "                                    transform_node=True, \n",
    "                                    transform_spatial=False):\n",
    "\n",
    "        '''\n",
    "        Input:\n",
    "            scene_embeddings: output of scene_graph module concatenated with pose. A list of of tensors containing node and\n",
    "                              spatial embeddings of each batch element\n",
    "            transform_node and transform spatial: boolean flags whether to do any transformation on nodes or not\n",
    "        Output:\n",
    "            scene_embeddings: viewpoint transformed embeddings\n",
    "        '''\n",
    "        for ind,spatial_embeddings in enumerate(scene_embeddings):\n",
    "            # Do viewpoint transformation on spatial embeddings\n",
    "            print(\"viewpoint transform on spatial embeddings\")\n",
    "            scene_embeddings[ind] = self.spatial_viewpoint_transformation(scene_embeddings[ind])\n",
    "\n",
    "        return scene_embeddings \n",
    "        \n",
    "    def forward(self,\n",
    "                feed_dict,\n",
    "                mode=\"node\",\n",
    "                rel_viewpoint=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            feed_dict: a dictionary containing list tensors containing images and bounding box data. \n",
    "            Each element of the feed_dict corresponds to one elment of the batch. \n",
    "            Inside each batch are contained [\"image\": Image tensor, \n",
    "                                             \"boxes\":Bounding box tensor,\n",
    "                                             bounding box\n",
    "                                            ]\n",
    "            mode: should be either 'node' or 'spatial' depending on what feature you want to extract\n",
    "        \"\"\"\n",
    "        num_batch = feed_dict[\"images\"].shape[0]\n",
    "        num_total_nodes = feed_dict[\"objects\"].sum().item()\n",
    "        \n",
    "        image_features = self.feature_extractor(feed_dict[\"images\"])\n",
    "        outputs = self.scene_graph(image_features, feed_dict[\"objects_boxes\"], feed_dict[\"objects\"], mode=mode)\n",
    "        \n",
    "        if mode==\"node\":\n",
    "            #Flatten the node embeddings\n",
    "#             node_features = outputs[0][0]\n",
    "#             for num in range(1,num_batch):\n",
    "#                 node_features = torch.cat([node_features, outputs[num][0]], dim =0) \n",
    "\n",
    "            return outputs\n",
    "        \n",
    "        if mode==\"spatial\" and rel_viewpoint is not None:\n",
    "            print(\"Ading viewpoint information to spatial features\")\n",
    "            outputs = self.merge_pose_with_scene_embeddings(outputs,rel_viewpoint)\n",
    "            outputs = self.do_viewpoint_transformation(outputs)\n",
    "        \n",
    "#         #Flattent the spatial embeddings\n",
    "#         spatial_features = outputs[0][1].reshape(-1,256)\n",
    "\n",
    "#         for num in range(1, num_batch):\n",
    "#             spatial_features = torch.cat([spatial_features, outputs[num][1].reshape(-1,256)], dim =0)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Mode** : Node Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_ = feed_dict_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_ = encoder(feed_dict_, mode=\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_viewpoint_ = metadata[\"rel_viewpoint\"]\n",
    "rel_viewpoint_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_ = encoder.feature_extractor(feed_dict_[\"images\"])\n",
    "scene_graph_output = encoder.scene_graph(image_features_, feed_dict_[\"objects_boxes\"], feed_dict_[\"objects\"], mode=\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ind = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([2, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scene_graph_output), scene_graph_output[batch_ind].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([2, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node_features_,), node_features_[batch_ind].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Mode** : Spatial Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ading viewpoint information to spatial features\n",
      "0\n",
      "Adding pose to spatial embeddings\n",
      "1\n",
      "Adding pose to spatial embeddings\n",
      "2\n",
      "Adding pose to spatial embeddings\n",
      "3\n",
      "Adding pose to spatial embeddings\n",
      "4\n",
      "Adding pose to spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n",
      "viewpoint transform on spatial embeddings\n"
     ]
    }
   ],
   "source": [
    "spatial_features_ = encoder(feed_dict_, mode=\"spatial\", rel_viewpoint= rel_viewpoint_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([2, 2, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spatial_features_), spatial_features_[batch_ind].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_k_ = feed_dict_k\n",
    "feed_dict_q_ = feed_dict_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_k_ = encoder(feed_dict_k_, mode=\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_q_ = encoder(feed_dict_q_, mode=\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_embeddings(output_k, output_q, mode = \"node\"):\n",
    "    \n",
    "    if mode==\"node\":\n",
    "        mode = 0\n",
    "    elif mode==\"spatial\":\n",
    "        mode = 1\n",
    "    else:\n",
    "        raise ValueError(\"Mode should be either node or spatial\")\n",
    "    \n",
    "    num_batch = len(output_k)\n",
    "    assert num_batch==len(output_q)   \n",
    "    \n",
    "    output_q_rearrange = []\n",
    "    \n",
    "    for batch_ind in range(num_batch):\n",
    "        \n",
    "        num_obj_in_batch = output_k[batch_ind].shape[0]\n",
    "        assert num_obj_in_batch==output_q[batch_ind].shape[0]\n",
    "        \n",
    "        #flatten the features - \n",
    "        output_k[batch_ind] = output_k[batch_ind].view(-1,256)\n",
    "        output_q[batch_ind] = output_q[batch_ind].view(-1,256)\n",
    "        \n",
    "        #form two pool for nearest neighbour search\n",
    "        pool_e = output_k[batch_ind].clone().detach().cpu()\n",
    "        pool_g = output_q[batch_ind].clone().detach().cpu()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            knn_e = NearestNeighbors(n_neighbors= num_obj_in_batch, metric=\"euclidean\")\n",
    "            knn_g = NearestNeighbors(n_neighbors= num_obj_in_batch, metric=\"euclidean\")\n",
    "\n",
    "            knn_g.fit(pool_g)\n",
    "            knn_e.fit(pool_e)\n",
    "            \n",
    "            paired = []\n",
    "            pairs = []\n",
    "            for index in range(num_obj_in_batch):  \n",
    "\n",
    "                #fit knn on each of the object \n",
    "                _, indices_e = knn_g.kneighbors(torch.reshape(pool_e[index], (1,-1)).detach().cpu())\n",
    "                indices_e = list(indices_e.flatten())\n",
    "                for e in indices_e:\n",
    "                    if e not in paired:\n",
    "                        paired.append(e)\n",
    "                        pairs.append([index,e])\n",
    "                        break\n",
    "        \n",
    "        print(pairs)\n",
    "        #rearranging the matched in output_q based on pair formed\n",
    "        pool_g_rearranged = torch.zeros(pool_e.shape[0], 256)\n",
    "        for pair in pairs:\n",
    "            pool_g_rearranged[pair[0]] = output_q[batch_ind][pair[1]]\n",
    "        \n",
    "        output_q[batch_ind] = pool_g_rearranged.cuda()\n",
    "        \n",
    "    return output_k, output_q    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [1, 1]]\n",
      "[[0, 0], [1, 1]]\n",
      "[[0, 0], [1, 1]]\n",
      "[[0, 0], [1, 1]]\n",
      "[[0, 0], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "rearranged_output_k, rearranged_output_q = pair_embeddings(output_k_, output_q_, mode = \"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearranged_output_k==output_k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearranged_output_q==output_q_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching the spatial embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_k__ = encoder(feed_dict_k_, mode=\"spatial\")\n",
    "output_q__ = encoder(feed_dict_q_, mode=\"spatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_,__ = pair_embeddings(output_k_, output_q_, mode = \"spatial\")\n",
    "\n",
    "\n",
    "#Code breaking resolve later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the embeddings across batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_features_across_batch(feature_list):\n",
    "\n",
    "    num_batch = len(feature_list)\n",
    "    features = feature_list[0].view(-1,256)\n",
    "    \n",
    "    for num in range(1,num_batch):\n",
    "        features = torch.cat([features, feature_list[num]], dim =0)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_output_k = stack_features_across_batch(rearranged_output_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_output_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_output_k[3] == rearranged_output_k[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "disco"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
