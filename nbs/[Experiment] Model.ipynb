{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../feed_dict.json\",\"rb\") as file:\n",
    "    feed_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_q = feed_dict\n",
    "feed_dict_k = feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "feed_dict_q[\"view\"] = torch.rand((feed_dict[\"image\"].size(0),1,7))\n",
    "feed_dict_k[\"view\"] = torch.rand((feed_dict[\"image\"].size(0),1,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.scene_graph.scene_graph import SceneGraph\n",
    "from torchvision.models import resnet34\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet34(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-3])\n",
    "scene_graph = SceneGraph(feature_dim=256, \n",
    "                         output_dims=[256,256],\n",
    "                         downsample_rate=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scene_graph(feed_dict):\n",
    "    image_features = feature_extractor(feed_dict[\"image\"])\n",
    "    outputs = scene_graph(image_features, feed_dict[\"objects\"], feed_dict[\"objects_length\"])\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pose_with_scene_embeddings(scene_embeddings, view):\n",
    "\n",
    "    for ind,(visual_embeddings, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "\n",
    "        # Broadcast view to visual embedding dimension\n",
    "        view_visual = view[ind].repeat(visual_embeddings.shape[0],1)\n",
    "        # Concatenate with visual embeddings\n",
    "        pose_with_features = torch.cat((view_visual,visual_embeddings), dim=1)\n",
    "        # Reassign the scene embeddings\n",
    "        scene_embeddings[ind][0] = pose_with_features\n",
    "\n",
    "        # Broadcast view to spatial embedding dimension\n",
    "        view_spatial = view[ind].unsqueeze(0).repeat(spatial_embeddings.shape[0],spatial_embeddings.shape[1],1)\n",
    "        # Concatenate with visual embeddings\n",
    "        pose_with_features = torch.cat((view_spatial,spatial_embeddings), dim=2)\n",
    "        # Reassign the scene embeddings\n",
    "        scene_embeddings[ind][1] = pose_with_features\n",
    "\n",
    "        ### To Do : Write some assertion test : (Saksham)\n",
    "    \n",
    "        return scene_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_embeddings_q = get_scene_graph(feed_dict_q)\n",
    "scene_embeddings_k = get_scene_graph(feed_dict_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_embeddings_q = merge_pose_with_scene_embeddings(scene_embeddings_q, feed_dict_q[\"view\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_embeddings_k = merge_pose_with_scene_embeddings(scene_embeddings_k, feed_dict_k[\"view\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_view_point_transformation_layer = nn.Sequential(nn.Linear(263,512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512,263))\n",
    "\n",
    "spatial_view_point_transformation_layer = nn.Sequential(nn.Linear(263,512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512,263))\n",
    "\n",
    "##### To DO : Reset Parameters of \"view_point_transformation_layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 263])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_point_transformation_layer(scene_embeddings_k[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_viewpoint_transformation(scene_embeddings):\n",
    "    \n",
    "    for ind,(visual_embeddings, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "        \n",
    "        # Do viewpoint transformation on visual embeddings\n",
    "        scene_embeddings[ind][0] = visual_view_point_transformation_layer(scene_embeddings_k[ind][0])\n",
    "        \n",
    "        # Do viewpoint transformation on spatial embeddings\n",
    "        scene_embeddings[ind][1] = spatial_view_point_transformation_layer(scene_embeddings_k[ind][1])\n",
    "        \n",
    "        return scene_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_embeddings_k_transformed = do_viewpoint_transformation(scene_embeddings_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 263])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_embeddings_k_transformed[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "        self.resnet = resnet34(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-3])\n",
    "        self.scene_graph = SceneGraph(feature_dim=256, \n",
    "                                 output_dims=[256,256],\n",
    "                                 downsample_rate=16)\n",
    "        \n",
    "        self.visual_view_point_transformation_layer = nn.Sequential(nn.Linear(263,512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512,263))\n",
    "\n",
    "        self.spatial_view_point_transformation_layer = nn.Sequential(nn.Linear(263,512),\n",
    "                                                        nn.ReLU(),\n",
    "                                                        nn.Linear(512,263))\n",
    "\n",
    "        ##### To DO : Reset Parameters of \"view_point_transformation_layer\"\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self,feed_dict_k, feed_dict_q):\n",
    "    \n",
    "        scene_embeddings_q = get_scene_graph(feed_dict_q)\n",
    "        scene_embeddings_k = get_scene_graph(feed_dict_k)\n",
    "        \n",
    "        scene_embeddings_q = merge_pose_with_scene_embeddings(scene_embeddings_q, feed_dict_q[\"view\"])\n",
    "        scene_embeddings_k = merge_pose_with_scene_embeddings(scene_embeddings_k, feed_dict_k[\"view\"])\n",
    "        \n",
    "        scene_embeddings_k_transformed = do_viewpoint_transformation(scene_embeddings_k)\n",
    "        \n",
    "        return scene_embeddings_q, scene_embeddings_q, scene_embeddings_k_transformed\n",
    "    \n",
    "\n",
    "    def get_scene_graph(self,feed_dict):\n",
    "        image_features = self.feature_extractor(feed_dict[\"image\"])\n",
    "        outputs = self.scene_graph(image_features, feed_dict[\"objects\"], feed_dict[\"objects_length\"])\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_pose_with_scene_embeddings(scene_embeddings, view):\n",
    "\n",
    "        for ind,(visual_embeddings, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "\n",
    "            # Broadcast view to visual embedding dimension\n",
    "            view_visual = view[ind].repeat(visual_embeddings.shape[0],1)\n",
    "            # Concatenate with visual embeddings\n",
    "            pose_with_features = torch.cat((view_visual,visual_embeddings), dim=1)\n",
    "            # Reassign the scene embeddings\n",
    "            scene_embeddings[ind][0] = pose_with_features\n",
    "\n",
    "            # Broadcast view to spatial embedding dimension\n",
    "            view_spatial = view[ind].unsqueeze(0).repeat(spatial_embeddings.shape[0],spatial_embeddings.shape[1],1)\n",
    "            # Concatenate with visual embeddings\n",
    "            pose_with_features = torch.cat((view_spatial,spatial_embeddings), dim=2)\n",
    "            # Reassign the scene embeddings\n",
    "            scene_embeddings[ind][1] = pose_with_features\n",
    "\n",
    "            ### To Do : Write some assertion test : (Saksham)\n",
    "\n",
    "            return scene_embeddings\n",
    "    \n",
    "    @staticmethod  \n",
    "    def do_viewpoint_transformation(scene_embeddings):\n",
    "    \n",
    "        for ind,(visual_embeddings, spatial_embeddings) in enumerate(scene_embeddings):\n",
    "\n",
    "            # Do viewpoint transformation on visual embeddings\n",
    "            scene_embeddings[ind][0] = visual_view_point_transformation_layer(scene_embeddings_k[ind][0])\n",
    "\n",
    "            # Do viewpoint transformation on spatial embeddings\n",
    "            scene_embeddings[ind][1] = spatial_view_point_transformation_layer(scene_embeddings_k[ind][1])\n",
    "\n",
    "            return scene_embeddings\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        pass\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_embeddings_q, scene_embeddings_q, scene_embeddings_k_transformed = m(feed_dict_k, feed_dict_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlg",
   "language": "python",
   "name": "dlg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
