{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloader import CLEVR_train, collate_boxes\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised..... 234  files...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CLEVR_train(root_dir='/home/mprabhud/dataset/clevr_lang/npys/ab_5t.txt')\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    feed_dict_q, feed_dict_k, metadata = b\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_q[\"images\"] = feed_dict_k[\"images\"].cuda()\n",
    "feed_dict_k[\"images\"] = feed_dict_k[\"images\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.scene_graph.scene_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import RoIAlign\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "from core.model.scene_graph.utils import *\n",
    "\n",
    "class SceneGraph(nn.Module):\n",
    "    def __init__(self, feature_dim=256, output_dims=[256,256], downsample_rate=16, mode=None):\n",
    "        super().__init__()\n",
    "        self.pool_size = 7\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dims = output_dims\n",
    "        self.downsample_rate = downsample_rate\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.object_roi_pool = RoIAlign(self.pool_size, 1.0 / self.downsample_rate, -1)\n",
    "#         self.context_roi_pool = RoIAlign(self.pool_size, 1.0 / self.downsample_rate, -1)\n",
    "        self.relation_roi_pool = RoIAlign(self.pool_size, 1.0 / self.downsample_rate, -1)\n",
    "        \n",
    "#         self.context_feature_extract = nn.Conv2d(feature_dim, feature_dim, 1)\n",
    "        self.relation_feature_extract = nn.Conv2d(feature_dim, feature_dim // 2 * 3, 1)\n",
    "\n",
    "#         self.object_feature_fuse = nn.Conv2d(feature_dim * 2, output_dims[0], 1)\n",
    "        self.relation_feature_fuse = nn.Conv2d(feature_dim // 2 * 3 + output_dims[0] * 2, output_dims[1], 1)\n",
    "\n",
    "        self.object_feature_fc = nn.Sequential(nn.ReLU(True), nn.Linear(output_dims[0] * self.pool_size ** 2, output_dims[0]))\n",
    "        self.relation_feature_fc = nn.Sequential(nn.ReLU(True), nn.Linear(output_dims[1] * self.pool_size ** 2, output_dims[1]))\n",
    "        \n",
    "        # this will change for models with multiple objects in future\n",
    "        # in that case, it will pick up the pretrained weights\n",
    "        if mode==\"node\":\n",
    "            self.reset_parameters()\n",
    "            \n",
    "        if self.mode==\"spatial\":\n",
    "            self.set_parameter_requires_grad()\n",
    "            \n",
    "    def set_parameter_requires_grad(self):\n",
    "        self.object_roi_pool.requires_grad = False\n",
    "        self.object_feature_fc.requires_grad = False\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, image_features, objects, objects_length):\n",
    "        \n",
    "        mode = self.mode\n",
    "        object_features = image_features\n",
    "#         context_features = self.context_feature_extract(image_features)\n",
    "        relation_features = self.relation_feature_extract(image_features)\n",
    "\n",
    "        outputs = list()\n",
    "        objects_index = 0\n",
    "        for i in range(image_features.size(0)):\n",
    "            box = objects[objects_index:objects_index + objects_length[i].item()]\n",
    "            objects_index += objects_length[i].item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_ind = i + torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device)\n",
    "\n",
    "                # generate a \"full-image\" bounding box\n",
    "                image_h, image_w = image_features.size(2) * self.downsample_rate, image_features.size(3) * self.downsample_rate\n",
    "                image_box = torch.cat([\n",
    "                    torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device),\n",
    "                    torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device),\n",
    "                    image_w + torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device),\n",
    "                    image_h + torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device)\n",
    "                ], dim=-1)\n",
    "\n",
    "                # meshgrid to obtain the subject and object bounding boxes\n",
    "                sub_id, obj_id = meshgrid(torch.arange(box.size(0), dtype=torch.int64, device=box.device), dim=0)\n",
    "                sub_id, obj_id = sub_id.contiguous().view(-1), obj_id.contiguous().view(-1)\n",
    "                sub_box, obj_box = meshgrid(box, dim=0)\n",
    "                sub_box = sub_box.contiguous().view(box.size(0) ** 2, 4)\n",
    "                obj_box = obj_box.contiguous().view(box.size(0) ** 2, 4)\n",
    "\n",
    "                # union box\n",
    "                union_box = generate_union_box(sub_box, obj_box)\n",
    "                rel_batch_ind = i + torch.zeros(union_box.size(0), 1, dtype=box.dtype, device=box.device)\n",
    "\n",
    "                # intersection maps\n",
    "#                 box_context_imap = generate_intersection_map(box, image_box, self.pool_size)\n",
    "                sub_union_imap = generate_intersection_map(sub_box, union_box, self.pool_size)\n",
    "                obj_union_imap = generate_intersection_map(obj_box, union_box, self.pool_size)\n",
    "\n",
    "#             this_context_features = self.context_roi_pool(context_features, torch.cat([batch_ind, image_box], dim=-1))\n",
    "#             x, y = this_context_features.chunk(2, dim=1)\n",
    "#             this_object_features = self.object_feature_fuse(torch.cat([\n",
    "#                 self.object_roi_pool(object_features, torch.cat([batch_ind, box], dim=-1)),\n",
    "#                 x, y * box_context_imap\n",
    "#             ], dim=1))\n",
    "            \n",
    "            this_object_features = self.object_roi_pool(object_features, torch.cat([batch_ind, box], dim=-1))\n",
    "            \n",
    "            if mode==\"node\":\n",
    "                outputs.append([\n",
    "                    self._norm(self.object_feature_fc(this_object_features.view(box.size(0), -1))), None\n",
    "                ])                \n",
    "            elif mode==\"spatial\":\n",
    "                this_relation_features = self.relation_roi_pool(relation_features, torch.cat([rel_batch_ind, union_box], dim=-1))\n",
    "                x, y, z = this_relation_features.chunk(3, dim=1)\n",
    "                this_relation_features = self.relation_feature_fuse(torch.cat([\n",
    "                    this_object_features[sub_id], this_object_features[obj_id],\n",
    "                    x, y * sub_union_imap, z * obj_union_imap\n",
    "                ], dim=1))\n",
    "\n",
    "                outputs.append([\n",
    "                    self._norm(self.object_feature_fc(this_object_features.view(box.size(0), -1))),\n",
    "                    self._norm(self.relation_feature_fc(this_relation_features.view(box.size(0) * box.size(0), -1)).view(box.size(0), box.size(0), -1))\n",
    "                ])\n",
    "            else:\n",
    "                raise ValueError(\"Feature Generation mode not defined properly. It should be either 'node' or 'spatial'.\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x / x.norm(2, dim=-1, keepdim=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet34(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-3])\n",
    "feature_extractor =  feature_extractor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_graph = SceneGraph(feature_dim=256, \n",
    "                         output_dims=[256,256],\n",
    "                         downsample_rate=16,\n",
    "                         mode=\"node\")\n",
    "scene_graph = scene_graph.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = feature_extractor(feed_dict_q[\"images\"])\n",
    "outputs = scene_graph(image_features, feed_dict_q[\"objects_boxes\"], feed_dict_q[\"objects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_graph = SceneGraph(feature_dim=256, \n",
    "                         output_dims=[256,256],\n",
    "                         downsample_rate=16,\n",
    "                         mode=\"spatial\")\n",
    "scene_graph = scene_graph.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = feature_extractor(feed_dict_q[\"images\"])\n",
    "outputs = scene_graph(image_features, feed_dict_q[\"objects_boxes\"], feed_dict_q[\"objects\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256]), torch.Size([2, 2, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0].shape, outputs[0][1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "disco"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
