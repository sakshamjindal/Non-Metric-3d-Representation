{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.cuda as cuda\n",
    "\n",
    "from jacinle.cli.argument import JacArgumentParser\n",
    "from jacinle.logging import get_logger, set_output_file\n",
    "from jacinle.utils.imp import load_source\n",
    "from jacinle.utils.tqdm import tqdm_pbar\n",
    "\n",
    "from jactorch.cli import escape_desc_name, ensure_path, dump_metainfo\n",
    "from jactorch.cuda.copy import async_copy_to\n",
    "from jactorch.train import TrainerEnv\n",
    "from jactorch.utils.meta import as_float\n",
    "\n",
    "from nscl.datasets import get_available_datasets, initialize_dataset, get_dataset_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JacArgumentParser(description=__doc__.strip())\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_dir = \"~/CLEVR_v1.0/val\"\n",
    "args.data_image_root =  \"~/CLEVR_v1.0/val/images/\"\n",
    "args.data_vocab_json =  \"~/CLEVR_v1.0/val/vocab.json\"\n",
    "args.data_scenes_json =  \"~/CLEVR_v1.0/val/scenes.json\"\n",
    "args.data_questions_json = \"~/CLEVR_v1.0/val/CLEVR_val_questions.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = \"clevr\"\n",
    "args.desc = \"experiments/clevr/desc_nscl_derender.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_dataset(args.dataset)\n",
    "build_dataset = get_dataset_builder(args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = load_source(args.desc)\n",
    "configs = desc.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(args, configs, args.data_image_root, args.data_scenes_json, args.data_questions_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dataset.make_dataloader(2, shuffle=True, drop_last=True, nr_workers=1)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = desc.make_model(args, dataset.unwrapped.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jacinle.utils.container import GView\n",
    "\n",
    "feed_dict = GView(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scene_graph.scene_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import RoIAlign\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "from .utils import *\n",
    "\n",
    "class SceneGraph(nn.Module):\n",
    "    def __init__(self, feature_dim=256, output_dims=[256,256], downsample_rate=16):\n",
    "        super().__init__()\n",
    "        self.pool_size = 7\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dims = output_dims\n",
    "        self.downsample_rate = downsample_rate\n",
    "        \n",
    "        self.object_roi_pool = RoIAlign(self.pool_size, 1.0 / self.downsample_rate, -1)\n",
    "        self.context_roi_pool = RoIAlign(self.pool_size, 1.0 / self.downsample_rate, -1)\n",
    "        self.relation_roi_pool = RoIAlign(self.pool_size, 1.0 / self.downsample_rate, -1)\n",
    "        \n",
    "        self.context_feature_extract = nn.Conv2d(feature_dim, feature_dim, 1)\n",
    "        self.relation_feature_extract = nn.Conv2d(feature_dim, feature_dim // 2 * 3, 1)\n",
    "\n",
    "        self.object_feature_fuse = nn.Conv2d(feature_dim * 2, output_dims[0], 1)\n",
    "        self.relation_feature_fuse = nn.Conv2d(feature_dim // 2 * 3 + output_dims[0] * 2, output_dims[1], 1)\n",
    "\n",
    "        self.object_feature_fc = nn.Sequential(nn.ReLU(True), nn.Linear(output_dims[0] * self.pool_size ** 2, output_dims[0]))\n",
    "        self.relation_feature_fc = nn.Sequential(nn.ReLU(True), nn.Linear(output_dims[1] * self.pool_size ** 2, output_dims[1]))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, image_features, objects, objects_length):\n",
    "        \n",
    "        object_features = image_features\n",
    "        context_features = self.context_feature_extract(image_features)\n",
    "        relation_features = self.relation_feature_extract(image_features)\n",
    "\n",
    "        outputs = list()\n",
    "        objects_index = 0\n",
    "        for i in range(image_features.size(0)):\n",
    "            box = objects[objects_index:objects_index + objects_length[i].item()]\n",
    "            objects_index += objects_length[i].item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_ind = i + torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device)\n",
    "\n",
    "                # generate a \"full-image\" bounding box\n",
    "                image_h, image_w = image_features.size(2) * self.downsample_rate, image_features.size(3) * self.downsample_rate\n",
    "                image_box = torch.cat([\n",
    "                    torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device),\n",
    "                    torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device),\n",
    "                    image_w + torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device),\n",
    "                    image_h + torch.zeros(box.size(0), 1, dtype=box.dtype, device=box.device)\n",
    "                ], dim=-1)\n",
    "\n",
    "                # meshgrid to obtain the subject and object bounding boxes\n",
    "                sub_id, obj_id = meshgrid(torch.arange(box.size(0), dtype=torch.int64, device=box.device), dim=0)\n",
    "                sub_id, obj_id = sub_id.contiguous().view(-1), obj_id.contiguous().view(-1)\n",
    "                sub_box, obj_box = meshgrid(box, dim=0)\n",
    "                sub_box = sub_box.contiguous().view(box.size(0) ** 2, 4)\n",
    "                obj_box = obj_box.contiguous().view(box.size(0) ** 2, 4)\n",
    "\n",
    "                # union box\n",
    "                union_box = generate_union_box(sub_box, obj_box)\n",
    "                rel_batch_ind = i + torch.zeros(union_box.size(0), 1, dtype=box.dtype, device=box.device)\n",
    "\n",
    "                # intersection maps\n",
    "                box_context_imap = generate_intersection_map(box, image_box, self.pool_size)\n",
    "                sub_union_imap = generate_intersection_map(sub_box, union_box, self.pool_size)\n",
    "                obj_union_imap = generate_intersection_map(obj_box, union_box, self.pool_size)\n",
    "\n",
    "            this_context_features = self.context_roi_pool(context_features, torch.cat([batch_ind, image_box], dim=-1))\n",
    "            x, y = this_context_features.chunk(2, dim=1)\n",
    "            this_object_features = self.object_feature_fuse(torch.cat([\n",
    "                self.object_roi_pool(object_features, torch.cat([batch_ind, box], dim=-1)),\n",
    "                x, y * box_context_imap\n",
    "            ], dim=1))\n",
    "\n",
    "            this_relation_features = self.relation_roi_pool(relation_features, torch.cat([rel_batch_ind, union_box], dim=-1))\n",
    "            x, y, z = this_relation_features.chunk(3, dim=1)\n",
    "            this_relation_features = self.relation_feature_fuse(torch.cat([\n",
    "                this_object_features[sub_id], this_object_features[obj_id],\n",
    "                x, y * sub_union_imap, z * obj_union_imap\n",
    "            ], dim=1))\n",
    "\n",
    "\n",
    "            outputs.append([\n",
    "                self._norm(self.object_feature_fc(this_object_features.view(box.size(0), -1))),\n",
    "                self._norm(self.relation_feature_fc(this_relation_features.view(box.size(0) * box.size(0), -1)).view(box.size(0), box.size(0), -1))\n",
    "            ])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x / x.norm(2, dim=-1, keepdim=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_graph = SceneGraph(feature_dim=256, \n",
    "                         output_dims=[256,256],\n",
    "                         downsample_rate=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet34(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = feature_extractor(feed_dict[\"image\"])\n",
    "outputs = scene_graph(image_features, feed_dict[\"objects\"], feed_dict[\"objects_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1][2].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlg",
   "language": "python",
   "name": "dlg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
